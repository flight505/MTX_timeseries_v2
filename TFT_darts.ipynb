{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Series Processing and Forecasting with Darts\n",
    "\n",
    "This notebook demonstrates a comprehensive pipeline for processing time series data and forecasting using the Darts library. The steps include data loading, preprocessing, splitting, model training, prediction, backtesting, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing import Pipeline\n",
    "from darts.dataprocessing.transformers import (\n",
    "    MissingValuesFiller,\n",
    "    Scaler,\n",
    "    StaticCovariatesTransformer,\n",
    ")\n",
    "from darts.metrics import mae, mape, r2_score, rmse\n",
    "from darts.models import TFTModel\n",
    "from darts.utils.statistics import check_seasonality, plot_acf\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from darts.explainability import TFTExplainer\n",
    "from darts.utils.likelihood_models import QuantileRegression\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import expit as sigmoid\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Logging and Suppress Warnings\n",
    "\n",
    "Configure the logging system to capture and display information throughout the pipeline execution. Suppress any non-critical warnings to keep the output clean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"timeseries_processing.log\"),\n",
    "        logging.StreamHandler(),\n",
    "    ],\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Prepare Data\n",
    "\n",
    "Load the preprocessed blood data from a CSV file and convert relevant columns to datetime objects. Ensure consistent data types across all columns for reliable processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 21:48:06,695 - INFO - Loading data...\n",
      "2024-10-24 21:48:06,890 - INFO - Converting date columns...\n",
      "2024-10-24 21:48:06,929 - INFO - Converting data types...\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Loading data...\")\n",
    "# Load the data\n",
    "df = pd.read_csv(\n",
    "    \"data/df_blood_preprocessed.csv\",\n",
    ")\n",
    "\n",
    "# Convert date columns to datetime\n",
    "logger.info(\"Converting date columns...\")\n",
    "df[\"ds\"] = pd.to_datetime(df[\"ds\"])\n",
    "df[\"first_infusion_date\"] = pd.to_datetime(df[\"first_infusion_date\"])\n",
    "df[\"next_infusion\"] = pd.to_datetime(df[\"next_infusion\"])\n",
    "\n",
    "# Define data types for consistency\n",
    "dtype_map = {\n",
    "    \"ds\": \"datetime64[ns]\",\n",
    "    \"first_infusion_date\": \"datetime64[ns]\",\n",
    "    \"next_infusion\": \"datetime64[ns]\",\n",
    "    \"unique_id\": str,\n",
    "    \"sex\": int,\n",
    "    \"age_at_diagdate\": float,\n",
    "    \"weight_change_cycles\": float,\n",
    "    \"component\": str,\n",
    "    \"value\": float,\n",
    "    \"unit\": str,\n",
    "    \"days_since_first_infusion\": int,\n",
    "    \"infno_day\": int,\n",
    "    \"infno\": int,\n",
    "}\n",
    "\n",
    "logger.info(\"Converting data types...\")\n",
    "df = df.astype(dtype_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anonymize Unique Patient IDs\n",
    "\n",
    "For privacy reasons, anonymize the unique_id column by converting numerical digits to corresponding letters. This ensures that patient identities are protected in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 21:48:06,969 - INFO - Anonymizing patient IDs...\n"
     ]
    }
   ],
   "source": [
    "def anonymize_unique_id(df):\n",
    "    logger.info(\"Anonymizing patient IDs...\")\n",
    "    digit_to_letter = {str(i): chr(ord(\"a\") + i - 1) for i in range(1, 10)}\n",
    "    digit_to_letter[\"0\"] = \"j\"\n",
    "    df[\"unique_id\"] = (\n",
    "        df[\"unique_id\"]\n",
    "        .astype(str)\n",
    "        .apply(lambda x: \"\".join(digit_to_letter.get(d, \"k\") for d in x))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df = anonymize_unique_id(df)\n",
    "df = df.drop(columns=[\"unit\", \"transer\", \"transth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Target Variable\n",
    "\n",
    "Specify the target column for forecasting. In this case, we aim to predict Neutrophilocytes_B.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMN = \"Neutrophilocytes_B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Series Preprocessor Class\n",
    "\n",
    "Define a TimeSeriesPreprocessor class to handle various preprocessing tasks such as handling missing values, encoding temporal information, and completing the timeline. This class ensures that the data is clean and ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesPreprocessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_column=None,\n",
    "        complete_timeline=False,\n",
    "        encode_temporal_distance=False,\n",
    "        add_decay=False,\n",
    "        fill_residual_nan=False,\n",
    "        exclude_columns=None,\n",
    "    ):\n",
    "        self.target_column = target_column\n",
    "        self.complete_timeline = complete_timeline\n",
    "        self.encode_temporal_distance = encode_temporal_distance\n",
    "        self.add_decay = add_decay\n",
    "        self.decay_params = {}\n",
    "        self.beta_params = {}\n",
    "        self.fill_residual_nan = fill_residual_nan\n",
    "        self.exclude_columns = exclude_columns or []\n",
    "        logger.info(\n",
    "            f\"Initialized TimeSeriesPreprocessor with target column: {target_column}\"\n",
    "        )\n",
    "\n",
    "    def _calculate_delta_matrix(self, df, col):\n",
    "        \"\"\"Calculate delta matrix according to Equation 1\"\"\"\n",
    "        logger.debug(f\"Calculating delta matrix for column: {col}\")\n",
    "        df = df.sort_values([\"unique_id\", \"normalized_time\"])\n",
    "        mask = (~df[col].isna()).astype(int)\n",
    "        time_diff = df.groupby(\"unique_id\")[\"normalized_time\"].diff()\n",
    "\n",
    "        delta = pd.Series(index=df.index, dtype=float)\n",
    "\n",
    "        for uid in tqdm(\n",
    "            df[\"unique_id\"].unique(), desc=f\"Processing delta matrix for {col}\"\n",
    "        ):\n",
    "            mask_group = mask[df[\"unique_id\"] == uid]\n",
    "            time_diff_group = time_diff[df[\"unique_id\"] == uid]\n",
    "            delta_group = pd.Series(index=mask_group.index, dtype=float)\n",
    "            delta_group.iloc[0] = 0\n",
    "\n",
    "            for t in range(1, len(mask_group)):\n",
    "                if mask_group.iloc[t - 1] == 0:\n",
    "                    delta_group.iloc[t] = (\n",
    "                        time_diff_group.iloc[t] + delta_group.iloc[t - 1]\n",
    "                    )\n",
    "                else:\n",
    "                    delta_group.iloc[t] = time_diff_group.iloc[t]\n",
    "\n",
    "            delta[delta_group.index] = delta_group\n",
    "\n",
    "        return delta\n",
    "\n",
    "    def _calculate_decay_parameter(self, delta, W_gamma=0.1, b_gamma=0.0):\n",
    "        \"\"\"Calculate temporal decay parameter gamma according to Equation 2\"\"\"\n",
    "        logger.debug(\"Calculating decay parameter\")\n",
    "        return np.exp(-np.maximum(0, W_gamma * delta + b_gamma))\n",
    "\n",
    "    def _initialize_missing_values(self, df, col, gamma):\n",
    "        \"\"\"Initialize missing values according to Equation 3\"\"\"\n",
    "        logger.debug(f\"Initializing missing values for column: {col}\")\n",
    "        mask = (~df[col].isna()).astype(int)\n",
    "        col_mean = df[col].mean()\n",
    "        last_observed = df.groupby(\"unique_id\")[col].ffill()\n",
    "        initialized_values = gamma * last_observed + (1 - gamma) * col_mean\n",
    "        df[col] = df[col].fillna(initialized_values)\n",
    "        return df\n",
    "\n",
    "    def _calculate_beta_parameter(self, gamma, mask, W_beta=0.1, b_beta=0.0):\n",
    "        \"\"\"Calculate trade-off parameter beta according to Equation 4\"\"\"\n",
    "        logger.debug(\"Calculating beta parameter\")\n",
    "        combined = np.column_stack([gamma, mask])\n",
    "        return sigmoid(np.dot(combined, W_beta) + b_beta)\n",
    "\n",
    "    def prepare_and_process(self, df):\n",
    "        logger.info(\"Starting data preparation and processing...\")\n",
    "        df = df.copy()\n",
    "        df = self._initial_preparation(df)\n",
    "\n",
    "        if self.complete_timeline:\n",
    "            logger.info(\"Completing timeline...\")\n",
    "            df = self._complete_timeline(df)\n",
    "\n",
    "        feature_columns = [\n",
    "            col\n",
    "            for col in df.columns\n",
    "            if col not in self.exclude_columns\n",
    "            and not col.endswith((\"_delta\", \"_missing\"))\n",
    "        ]\n",
    "\n",
    "        logger.info(\"Processing features with temporal knowledge...\")\n",
    "        for col in tqdm(feature_columns, desc=\"Processing features\"):\n",
    "            if df[col].isna().any():\n",
    "                delta = self._calculate_delta_matrix(df, col)\n",
    "                gamma = self._calculate_decay_parameter(delta)\n",
    "                df = self._initialize_missing_values(df, col, gamma)\n",
    "                mask = (~df[col].isna()).astype(int)\n",
    "                beta = self._calculate_beta_parameter(gamma, mask)\n",
    "                self.decay_params[col] = {\"gamma\": gamma, \"delta\": delta}\n",
    "                self.beta_params[col] = beta\n",
    "\n",
    "        if self.encode_temporal_distance:\n",
    "            logger.info(\"Encoding temporal distance...\")\n",
    "            df = self._encode_temporal_distance(df)\n",
    "\n",
    "        if self.fill_residual_nan:\n",
    "            logger.info(\"Filling residual NaN values...\")\n",
    "            df = self._fill_residual_nan(df)\n",
    "\n",
    "        logger.info(\"Data preparation and processing completed\")\n",
    "        return df\n",
    "\n",
    "    def _initial_preparation(self, X):\n",
    "        logger.info(\"Performing initial data preparation...\")\n",
    "        X = self._aggregate_daily(X)\n",
    "        X[\"ds_date\"] = pd.to_datetime(X[\"ds_date\"])\n",
    "        X[\"normalized_time\"] = X.groupby(\"unique_id\")[\"ds_date\"].transform(\n",
    "            lambda x: (x - x.min()).dt.days\n",
    "        )\n",
    "\n",
    "        # Calculate infno_day here\n",
    "        X[\"infno_first_time\"] = X.groupby([\"unique_id\", \"infno\"])[\n",
    "            \"normalized_time\"\n",
    "        ].transform(\"min\")\n",
    "        X[\"infno_day\"] = X[\"normalized_time\"] - X[\"infno_first_time\"]\n",
    "        X.drop(columns=[\"infno_first_time\"], inplace=True)\n",
    "\n",
    "        X = X.pivot_table(\n",
    "            index=[\n",
    "                \"unique_id\",\n",
    "                \"normalized_time\",\n",
    "                \"age_at_diagdate\",\n",
    "                \"sex\",\n",
    "                \"infno\",\n",
    "                \"infno_day\",\n",
    "                \"ds_date\",\n",
    "            ],\n",
    "            columns=\"component\",\n",
    "            values=\"value\",\n",
    "            aggfunc=\"first\",\n",
    "        ).reset_index()\n",
    "\n",
    "        if self.target_column in X.columns:\n",
    "            X = X.rename(columns={self.target_column: \"y\"})\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Target column '{self.target_column}' not found in DataFrame.\"\n",
    "            )\n",
    "\n",
    "        return X\n",
    "\n",
    "    def _aggregate_daily(self, X):\n",
    "        logger.info(\"Aggregating daily data...\")\n",
    "        X[\"ds_date\"] = X[\"ds\"].dt.date\n",
    "        grouped = X.groupby(\n",
    "            [\"unique_id\", \"ds_date\", \"age_at_diagdate\", \"sex\", \"infno\", \"component\"]\n",
    "        )\n",
    "        daily_data = grouped.agg({\"value\": \"mean\"}).reset_index()\n",
    "        return daily_data\n",
    "\n",
    "    def _complete_timeline(self, df):\n",
    "        logger.info(\"Completing timeline...\")\n",
    "        unique_ids = df[\"unique_id\"].unique()\n",
    "        complete_timeline = []\n",
    "\n",
    "        for uid in tqdm(unique_ids, desc=\"Processing timelines\"):\n",
    "            uid_df = df[df[\"unique_id\"] == uid]\n",
    "            min_time = uid_df[\"normalized_time\"].min()\n",
    "            max_time = uid_df[\"normalized_time\"].max()\n",
    "            timeline = pd.DataFrame(\n",
    "                {\"unique_id\": uid, \"normalized_time\": np.arange(min_time, max_time + 1)}\n",
    "            )\n",
    "            complete_timeline.append(timeline)\n",
    "\n",
    "        complete_timeline = pd.concat(complete_timeline, ignore_index=True)\n",
    "        df = pd.merge(\n",
    "            complete_timeline, df, on=[\"unique_id\", \"normalized_time\"], how=\"left\"\n",
    "        )\n",
    "\n",
    "        static_columns = [\"unique_id\", \"age_at_diagdate\", \"sex\"]\n",
    "        for col in static_columns:\n",
    "            df[col] = df.groupby(\"unique_id\")[col].ffill().bfill()\n",
    "\n",
    "        dynamic_columns = [\"infno\", \"infno_day\"]\n",
    "        for col in dynamic_columns:\n",
    "            df[col] = df.groupby(\"unique_id\")[col].ffill()\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _encode_temporal_distance(self, df):\n",
    "        logger.info(\"Encoding temporal distance...\")\n",
    "        feature_columns = [\n",
    "            col\n",
    "            for col in df.columns\n",
    "            if col not in self.exclude_columns and not col.endswith(\"_delta\")\n",
    "        ]\n",
    "        temporal_distance_data = {}\n",
    "\n",
    "        for col in tqdm(feature_columns, desc=\"Processing temporal distances\"):\n",
    "            if not col.endswith(\"_delta\"):\n",
    "                temporal_distance_series = (\n",
    "                    df.groupby(\"unique_id\")[col]\n",
    "                    .apply(\n",
    "                        lambda x: x.isna()\n",
    "                        .astype(int)\n",
    "                        .groupby(x.notna().astype(int).cumsum())\n",
    "                        .cumsum()\n",
    "                    )\n",
    "                    .astype(float)\n",
    "                )\n",
    "                temporal_distance_data[col + \"_delta\"] = temporal_distance_series\n",
    "\n",
    "        temporal_distance_df = pd.DataFrame(temporal_distance_data)\n",
    "        temporal_distance_df = temporal_distance_df.set_index(df.index)\n",
    "        df = pd.concat([df, temporal_distance_df], axis=1)\n",
    "        return df\n",
    "\n",
    "    def _fill_residual_nan(self, df):\n",
    "        logger.info(\"Filling residual NaN values...\")\n",
    "        feature_columns = [col for col in df.columns if col not in [\"unique_id\", \"ds\"]]\n",
    "        df[feature_columns] = df.groupby(\"unique_id\")[feature_columns].ffill()\n",
    "        df[feature_columns] = df.groupby(\"unique_id\")[feature_columns].bfill()\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize and Execute Preprocessor\n",
    "\n",
    "Create an instance of the TimeSeriesPreprocessor with specified parameters and process the dataset to prepare it for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 21:48:07,197 - INFO - Initializing preprocessor...\n",
      "2024-10-24 21:48:07,197 - INFO - Initialized TimeSeriesPreprocessor with target column: Neutrophilocytes_B\n",
      "2024-10-24 21:48:07,198 - INFO - Processing data...\n",
      "2024-10-24 21:48:07,198 - INFO - Starting data preparation and processing...\n",
      "2024-10-24 21:48:07,207 - INFO - Performing initial data preparation...\n",
      "2024-10-24 21:48:07,208 - INFO - Aggregating daily data...\n",
      "2024-10-24 21:48:07,409 - INFO - Completing timeline...\n",
      "2024-10-24 21:48:07,409 - INFO - Completing timeline...\n",
      "Processing timelines: 100%|██████████| 182/182 [00:00<00:00, 1404.81it/s]\n",
      "2024-10-24 21:48:07,605 - INFO - Processing features with temporal knowledge...\n",
      "Processing delta matrix for Alanine_transaminase_ALAT: 100%|██████████| 182/182 [00:02<00:00, 89.82it/s]\n",
      "Processing delta matrix for Albumin_P: 100%|██████████| 182/182 [00:02<00:00, 84.24it/s]\n",
      "Processing delta matrix for Alkaline_phosphatase_P: 100%|██████████| 182/182 [00:02<00:00, 83.94it/s]\n",
      "Processing delta matrix for Amylase_P: 100%|██████████| 182/182 [00:02<00:00, 83.31it/s]\n",
      "Processing delta matrix for Amylase_P_pancreatic_type: 100%|██████████| 182/182 [00:02<00:00, 78.96it/s]\n",
      "Processing delta matrix for Antitrombin_P: 100%|██████████| 182/182 [00:03<00:00, 52.34it/s]\n",
      "Processing delta matrix for Aspartate_transaminase_ASAT: 100%|██████████| 182/182 [00:03<00:00, 58.89it/s]\n",
      "Processing delta matrix for Basophilocytes_P: 100%|██████████| 182/182 [00:02<00:00, 60.91it/s]\n",
      "Processing delta matrix for Bilirubin_P: 100%|██████████| 182/182 [00:02<00:00, 64.60it/s]\n",
      "Processing delta matrix for Blast_cells_B: 100%|██████████| 182/182 [00:02<00:00, 84.13it/s]\n",
      "Processing delta matrix for C_reactive_protein: 100%|██████████| 182/182 [00:02<00:00, 84.71it/s]\n",
      "Processing delta matrix for Calcium_P: 100%|██████████| 182/182 [00:02<00:00, 83.30it/s]\n",
      "Processing delta matrix for Calcium_P_Adjusted: 100%|██████████| 182/182 [00:02<00:00, 83.99it/s]\n",
      "Processing delta matrix for Calcium_ion: 100%|██████████| 182/182 [00:02<00:00, 84.29it/s]\n",
      "Processing delta matrix for Carbamide_P: 100%|██████████| 182/182 [00:02<00:00, 83.51it/s]\n",
      "Processing delta matrix for Chloride_P: 100%|██████████| 182/182 [00:02<00:00, 83.49it/s]\n",
      "Processing delta matrix for Cholesterol_ester_P: 100%|██████████| 182/182 [00:02<00:00, 84.49it/s]\n",
      "Processing delta matrix for Cholesterol_ester_in_HDL: 100%|██████████| 182/182 [00:02<00:00, 83.17it/s]\n",
      "Processing delta matrix for Cholesterol_ester_in_LDL: 100%|██████████| 182/182 [00:02<00:00, 82.88it/s]\n",
      "Processing delta matrix for Coagulation_factor_II_VII_X_PP: 100%|██████████| 182/182 [00:02<00:00, 83.90it/s]\n",
      "Processing delta matrix for Coagulation_surface_induced_APTT: 100%|██████████| 182/182 [00:02<00:00, 84.65it/s]\n",
      "Processing delta matrix for Coagulation_tissue_factor_induced_INR: 100%|██████████| 182/182 [00:02<00:00, 84.31it/s]\n",
      "Processing delta matrix for Creatininium_P: 100%|██████████| 182/182 [00:02<00:00, 85.01it/s]\n",
      "Processing delta matrix for Eosinophilocytes_B: 100%|██████████| 182/182 [00:02<00:00, 85.65it/s]\n",
      "Processing delta matrix for Erythrocyte_Volume_Fraction_EVF: 100%|██████████| 182/182 [00:02<00:00, 84.26it/s]\n",
      "Processing delta matrix for Erythrocytes_B: 100%|██████████| 182/182 [00:02<00:00, 83.33it/s]\n",
      "Processing delta matrix for Erythrocytes_mean_corpuscular_volume_MCV: 100%|██████████| 182/182 [00:02<00:00, 83.59it/s]\n",
      "Processing delta matrix for Ferritin: 100%|██████████| 182/182 [00:02<00:00, 83.79it/s]\n",
      "Processing delta matrix for Fibrin_D_Dimer_P: 100%|██████████| 182/182 [00:02<00:00, 84.21it/s]\n",
      "Processing delta matrix for Fibrinogen_P: 100%|██████████| 182/182 [00:02<00:00, 82.60it/s]\n",
      "Processing delta matrix for Glucose_Csv: 100%|██████████| 182/182 [00:02<00:00, 84.59it/s]\n",
      "Processing delta matrix for Glucose_concentration_pt_fasting_status_unknown: 100%|██████████| 182/182 [00:02<00:00, 84.72it/s]\n",
      "Processing delta matrix for Hemoglobin_B: 100%|██████████| 182/182 [00:02<00:00, 85.14it/s]\n",
      "Processing delta matrix for Hemoglobin_HbA1c: 100%|██████████| 182/182 [00:02<00:00, 84.08it/s]\n",
      "Processing delta matrix for Hemoglobin_MCHC: 100%|██████████| 182/182 [00:02<00:00, 83.43it/s]\n",
      "Processing delta matrix for Hydrogencarbonat: 100%|██████████| 182/182 [00:02<00:00, 83.54it/s]\n",
      "Processing delta matrix for Immunoglobulin_A: 100%|██████████| 182/182 [00:02<00:00, 84.04it/s]\n",
      "Processing delta matrix for Immunoglobulin_G: 100%|██████████| 182/182 [00:02<00:00, 82.92it/s]\n",
      "Processing delta matrix for Immunoglobulin_M: 100%|██████████| 182/182 [00:02<00:00, 83.38it/s]\n",
      "Processing delta matrix for Lactate_dehydrogenase_LDH: 100%|██████████| 182/182 [00:02<00:00, 84.68it/s]\n",
      "Processing delta matrix for Leukocytes_B: 100%|██████████| 182/182 [00:02<00:00, 84.63it/s]\n",
      "Processing delta matrix for Lymphocytes_B: 100%|██████████| 182/182 [00:02<00:00, 85.53it/s]\n",
      "Processing delta matrix for Magnesium: 100%|██████████| 182/182 [00:02<00:00, 85.52it/s]\n",
      "Processing delta matrix for Metamyelocytes_B: 100%|██████████| 182/182 [00:02<00:00, 83.88it/s]\n",
      "Processing delta matrix for Metamyelocytes_Myelocytes_Promyelocytes_B: 100%|██████████| 182/182 [00:02<00:00, 84.69it/s]\n",
      "Processing delta matrix for Methotrexate_P: 100%|██████████| 182/182 [00:02<00:00, 85.30it/s]\n",
      "Processing delta matrix for Monocytes_B: 100%|██████████| 182/182 [00:02<00:00, 84.86it/s]\n",
      "Processing delta matrix for Myelocytes_B: 100%|██████████| 182/182 [00:02<00:00, 84.37it/s]\n",
      "Processing delta matrix for y: 100%|██████████| 182/182 [00:02<00:00, 85.25it/s]\n",
      "Processing delta matrix for Phosphate_P: 100%|██████████| 182/182 [00:02<00:00, 83.61it/s]\n",
      "Processing delta matrix for Plasmocytes: 100%|██████████| 182/182 [00:02<00:00, 83.04it/s]\n",
      "Processing delta matrix for Potassium_ion_P: 100%|██████████| 182/182 [00:02<00:00, 84.58it/s]\n",
      "Processing delta matrix for Procalcitonin_PCT: 100%|██████████| 182/182 [00:02<00:00, 83.16it/s]\n",
      "Processing delta matrix for Promyelocytes_B: 100%|██████████| 182/182 [00:02<00:00, 82.70it/s]\n",
      "Processing delta matrix for Protein_cerebrospinal_fluid_CSF: 100%|██████████| 182/182 [00:02<00:00, 83.45it/s]\n",
      "Processing delta matrix for Reticulocytes_total_B: 100%|██████████| 182/182 [00:02<00:00, 84.17it/s]\n",
      "Processing delta matrix for Sodium_ion_P: 100%|██████████| 182/182 [00:02<00:00, 83.98it/s]\n",
      "Processing delta matrix for Thrombocytes_B: 100%|██████████| 182/182 [00:02<00:00, 84.85it/s]\n",
      "Processing delta matrix for Thyrotropin_TSH: 100%|██████████| 182/182 [00:02<00:00, 84.54it/s]\n",
      "Processing delta matrix for Triacylglycerol_lipase: 100%|██████████| 182/182 [00:02<00:00, 82.78it/s]\n",
      "Processing delta matrix for Triglycerides_P_pt_fasting_status_unknown: 100%|██████████| 182/182 [00:02<00:00, 83.82it/s]\n",
      "Processing delta matrix for Urate: 100%|██████████| 182/182 [00:02<00:00, 84.45it/s]\n",
      "Processing delta matrix for eGFR_DSKB_DNS_2009: 100%|██████████| 182/182 [00:02<00:00, 83.98it/s]\n",
      "Processing features: 100%|██████████| 63/63 [02:22<00:00,  2.27s/it]\n",
      "2024-10-24 21:50:30,458 - INFO - Encoding temporal distance...\n",
      "2024-10-24 21:50:30,459 - INFO - Encoding temporal distance...\n",
      "Processing temporal distances: 100%|██████████| 63/63 [00:02<00:00, 23.62it/s]\n",
      "2024-10-24 21:50:33,441 - INFO - Filling residual NaN values...\n",
      "2024-10-24 21:50:33,441 - INFO - Filling residual NaN values...\n",
      "2024-10-24 21:50:33,506 - INFO - Data preparation and processing completed\n"
     ]
    }
   ],
   "source": [
    "# Initialize preprocessor\n",
    "logger.info(\"Initializing preprocessor...\")\n",
    "preprocessor = TimeSeriesPreprocessor(\n",
    "    target_column=TARGET_COLUMN,\n",
    "    complete_timeline=True,\n",
    "    encode_temporal_distance=True,\n",
    "    add_decay=True,\n",
    "    fill_residual_nan=True,\n",
    "    exclude_columns=[\n",
    "        \"unique_id\",\n",
    "        \"normalized_time\",\n",
    "        \"sex\",\n",
    "        \"age_at_diagdate\",\n",
    "        \"infno\",\n",
    "        \"infno_day\",\n",
    "        \"ds_date\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Process the data\n",
    "logger.info(\"Processing data...\")\n",
    "processed_df = preprocessor.prepare_and_process(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data\n",
    "processed_df.to_csv(\"data/processed_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>normalized_time</th>\n",
       "      <th>age_at_diagdate</th>\n",
       "      <th>sex</th>\n",
       "      <th>infno</th>\n",
       "      <th>infno_day</th>\n",
       "      <th>ds_date</th>\n",
       "      <th>Alanine_transaminase_ALAT</th>\n",
       "      <th>Albumin_P</th>\n",
       "      <th>Alkaline_phosphatase_P</th>\n",
       "      <th>...</th>\n",
       "      <th>Promyelocytes_B_delta</th>\n",
       "      <th>Protein_cerebrospinal_fluid_CSF_delta</th>\n",
       "      <th>Reticulocytes_total_B_delta</th>\n",
       "      <th>Sodium_ion_P_delta</th>\n",
       "      <th>Thrombocytes_B_delta</th>\n",
       "      <th>Thyrotropin_TSH_delta</th>\n",
       "      <th>Triacylglycerol_lipase_delta</th>\n",
       "      <th>Triglycerides_P_pt_fasting_status_unknown_delta</th>\n",
       "      <th>Urate_delta</th>\n",
       "      <th>eGFR_DSKB_DNS_2009_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bjaaaaa</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-05-16</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bjaaaaa</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2011-05-17</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>31.997028</td>\n",
       "      <td>90.455602</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bjaaaaa</td>\n",
       "      <td>2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2011-05-18</td>\n",
       "      <td>140.333333</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bjaaaaa</td>\n",
       "      <td>3</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2011-05-18</td>\n",
       "      <td>142.288006</td>\n",
       "      <td>30.187353</td>\n",
       "      <td>80.502391</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bjaaaaa</td>\n",
       "      <td>4</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2011-05-18</td>\n",
       "      <td>144.056667</td>\n",
       "      <td>30.356876</td>\n",
       "      <td>89.100509</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id  normalized_time  age_at_diagdate  sex  infno  infno_day  \\\n",
       "0   bjaaaaa                0             14.0  2.0    1.0        0.0   \n",
       "1   bjaaaaa                1             14.0  2.0    1.0        1.0   \n",
       "2   bjaaaaa                2             14.0  2.0    1.0        2.0   \n",
       "3   bjaaaaa                3             14.0  2.0    1.0        2.0   \n",
       "4   bjaaaaa                4             14.0  2.0    1.0        2.0   \n",
       "\n",
       "     ds_date  Alanine_transaminase_ALAT  Albumin_P  Alkaline_phosphatase_P  \\\n",
       "0 2011-05-16                 238.000000  32.000000               82.000000   \n",
       "1 2011-05-17                 162.000000  31.997028               90.455602   \n",
       "2 2011-05-18                 140.333333  30.000000               71.000000   \n",
       "3 2011-05-18                 142.288006  30.187353               80.502391   \n",
       "4 2011-05-18                 144.056667  30.356876               89.100509   \n",
       "\n",
       "   ...  Promyelocytes_B_delta  Protein_cerebrospinal_fluid_CSF_delta  \\\n",
       "0  ...                    1.0                                    1.0   \n",
       "1  ...                    2.0                                    2.0   \n",
       "2  ...                    3.0                                    3.0   \n",
       "3  ...                    4.0                                    4.0   \n",
       "4  ...                    5.0                                    5.0   \n",
       "\n",
       "   Reticulocytes_total_B_delta  Sodium_ion_P_delta  Thrombocytes_B_delta  \\\n",
       "0                          1.0                 0.0                   0.0   \n",
       "1                          2.0                 0.0                   0.0   \n",
       "2                          3.0                 0.0                   0.0   \n",
       "3                          4.0                 0.0                   0.0   \n",
       "4                          5.0                 0.0                   0.0   \n",
       "\n",
       "   Thyrotropin_TSH_delta  Triacylglycerol_lipase_delta  \\\n",
       "0                    1.0                           1.0   \n",
       "1                    2.0                           2.0   \n",
       "2                    3.0                           3.0   \n",
       "3                    4.0                           4.0   \n",
       "4                    5.0                           5.0   \n",
       "\n",
       "   Triglycerides_P_pt_fasting_status_unknown_delta  Urate_delta  \\\n",
       "0                                              1.0          1.0   \n",
       "1                                              2.0          2.0   \n",
       "2                                              3.0          3.0   \n",
       "3                                              4.0          4.0   \n",
       "4                                              5.0          5.0   \n",
       "\n",
       "   eGFR_DSKB_DNS_2009_delta  \n",
       "0                       1.0  \n",
       "1                       2.0  \n",
       "2                       3.0  \n",
       "3                       4.0  \n",
       "4                       5.0  \n",
       "\n",
       "[5 rows x 133 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# head\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data into Train, Validation, and Test Sets\n",
    "\n",
    "Define a function to split the unique patient IDs into training, validation, and test sets based on specified proportions. This ensures that the model is evaluated on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split data into train, validation, and test sets\n",
    "def split_data(unique_ids, test_size=0.2, val_size=0.1):\n",
    "    logger.info(\"Splitting data into train, validation, and test sets...\")\n",
    "    np.random.seed(42)\n",
    "    unique_ids = np.sort(unique_ids)\n",
    "    n_samples = len(unique_ids)\n",
    "    n_test = int(n_samples * test_size)\n",
    "    n_val = int(n_samples * val_size)\n",
    "    test_ids = unique_ids[-n_test:]\n",
    "    val_ids = unique_ids[-(n_test + n_val) : -n_test]\n",
    "    train_ids = unique_ids[: -(n_test + n_val)]\n",
    "    return train_ids.tolist(), val_ids.tolist(), test_ids.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create TimeSeries Objects\n",
    "\n",
    "Define a function to convert patient data into Darts TimeSeries objects, including static and past covariates. This is essential for feeding the data into the forecasting model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create TimeSeries objects\n",
    "def create_time_series(patient_data, value_cols, static_covariates):\n",
    "    logger.debug(f\"Creating time series for columns: {value_cols}\")\n",
    "    patient_data = patient_data.copy()\n",
    "    patient_data[\"date\"] = pd.to_datetime(patient_data[\"normalized_time\"], unit=\"D\")\n",
    "    patient_data = patient_data.sort_values(\"date\")\n",
    "\n",
    "    ts = TimeSeries.from_dataframe(\n",
    "        df=patient_data,\n",
    "        time_col=\"date\",\n",
    "        value_cols=value_cols,\n",
    "        static_covariates=static_covariates,\n",
    "        fill_missing_dates=True,\n",
    "        freq=\"D\",\n",
    "        fillna_value=None,\n",
    "    )\n",
    "\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Data for Model Training\n",
    "\n",
    "Split the unique patient IDs into training, validation, and test sets. Then, create TimeSeries objects for each patient and organize them into corresponding lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 21:50:33,591 - INFO - Preparing data for model training...\n",
      "2024-10-24 21:50:33,593 - INFO - Splitting data into train, validation, and test sets...\n",
      "2024-10-24 21:50:33,594 - INFO - Creating time series for each patient...\n",
      "Processing patients: 100%|██████████| 182/182 [00:01<00:00, 149.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for model training\n",
    "logger.info(\"Preparing data for model training...\")\n",
    "unique_ids = processed_df[\"unique_id\"].unique()\n",
    "train_ids, val_ids, test_ids = split_data(unique_ids)\n",
    "\n",
    "# Initialize lists for storing time series\n",
    "train_list, val_list, test_list = [], [], []\n",
    "train_covariates_list, val_covariates_list, test_covariates_list = [], [], []\n",
    "\n",
    "logger.info(\"Creating time series for each patient...\")\n",
    "for unique_id in tqdm(unique_ids, desc=\"Processing patients\"):\n",
    "    patient_data = processed_df[processed_df[\"unique_id\"] == unique_id]\n",
    "    static_covariates = patient_data[[\"sex\", \"age_at_diagdate\"]].iloc[0]\n",
    "    value_cols = [\"y\"]\n",
    "\n",
    "    # Create main series\n",
    "    patient_series = create_time_series(patient_data, value_cols, static_covariates)\n",
    "\n",
    "    # Create past covariates series\n",
    "    covariate_cols = [\"infno\", \"infno_day\"]\n",
    "    covariate_series = create_time_series(patient_data, covariate_cols, None)\n",
    "\n",
    "    # Append to appropriate list\n",
    "    if unique_id in train_ids:\n",
    "        train_list.append(patient_series)\n",
    "        train_covariates_list.append(covariate_series)\n",
    "    elif unique_id in val_ids:\n",
    "        val_list.append(patient_series)\n",
    "        val_covariates_list.append(covariate_series)\n",
    "    else:\n",
    "        test_list.append(patient_series)\n",
    "        test_covariates_list.append(covariate_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert TimeSeries to Float32\n",
    "\n",
    "Ensure that all TimeSeries objects are in float32 format for compatibility and optimized performance during model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 21:50:34,820 - INFO - Converting series to float32...\n"
     ]
    }
   ],
   "source": [
    "# Convert series to float32\n",
    "logger.info(\"Converting series to float32...\")\n",
    "\n",
    "def convert_to_float32(ts_list):\n",
    "    return [ts.astype(np.float32) for ts in ts_list]\n",
    "\n",
    "train_list = convert_to_float32(train_list)\n",
    "val_list = convert_to_float32(val_list)\n",
    "test_list = convert_to_float32(test_list)\n",
    "train_covariates_list = convert_to_float32(train_covariates_list)\n",
    "val_covariates_list = convert_to_float32(val_covariates_list)\n",
    "test_covariates_list = convert_to_float32(test_covariates_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the Data\n",
    "\n",
    "Apply scaling to both the main time series and covariates using Darts’ Pipeline and Scaler. Scaling ensures that the data is normalized, which can improve model training efficiency and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 21:50:34,971 - INFO - Creating scaling pipelines...\n",
      "2024-10-24 21:50:34,972 - INFO - Scaling the data...\n",
      "2024-10-24 21:50:35,100 - WARNING - Only 18 TimeSeries (lists) were provided which is lower than the number of series (n=128) used to fit Scaler. This can result in a mismatch between the series and the underlying transformers.\n",
      "2024-10-24 21:50:35,118 - WARNING - Only 36 TimeSeries (lists) were provided which is lower than the number of series (n=128) used to fit Scaler. This can result in a mismatch between the series and the underlying transformers.\n",
      "2024-10-24 21:50:35,298 - WARNING - Only 18 TimeSeries (lists) were provided which is lower than the number of series (n=128) used to fit Scaler. This can result in a mismatch between the series and the underlying transformers.\n",
      "2024-10-24 21:50:35,309 - WARNING - Only 36 TimeSeries (lists) were provided which is lower than the number of series (n=128) used to fit Scaler. This can result in a mismatch between the series and the underlying transformers.\n"
     ]
    }
   ],
   "source": [
    "# Create pipelines for scaling\n",
    "logger.info(\"Creating scaling pipelines...\")\n",
    "pipeline_main = Pipeline([Scaler()])\n",
    "pipeline_covariates = Pipeline([Scaler()])\n",
    "\n",
    "# Scale the data\n",
    "logger.info(\"Scaling the data...\")\n",
    "train_scaled = pipeline_main.fit_transform(train_list)\n",
    "val_scaled = pipeline_main.transform(val_list)\n",
    "test_scaled = pipeline_main.transform(test_list)\n",
    "\n",
    "train_covariates_scaled = pipeline_covariates.fit_transform(train_covariates_list)\n",
    "val_covariates_scaled = pipeline_covariates.transform(val_covariates_list)\n",
    "test_covariates_scaled = pipeline_covariates.transform(test_covariates_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for Missing Values\n",
    "\n",
    "Verify that there are no missing values (NaNs) in the scaled datasets. Presence of NaNs can lead to errors during model training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 21:50:35,330 - INFO - Checking for NaNs in train_scaled...\n",
      "2024-10-24 21:50:35,357 - INFO - Checking for NaNs in val_scaled...\n",
      "2024-10-24 21:50:35,362 - INFO - Checking for NaNs in test_scaled...\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs in scaled data\n",
    "def check_for_nans(series_list, name=\"series\"):\n",
    "    logger.info(f\"Checking for NaNs in {name}...\")\n",
    "    for i, series in enumerate(series_list):\n",
    "        if series.pd_dataframe().isna().any().any():\n",
    "            raise ValueError(f\"NaNs found in {name} at index {i}\")\n",
    "\n",
    "# Check scaled data\n",
    "check_for_nans(train_scaled, \"train_scaled\")\n",
    "check_for_nans(val_scaled, \"val_scaled\")\n",
    "check_for_nans(test_scaled, \"test_scaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Datetime Features\n",
    "\n",
    "Generate additional datetime-based features such as month, year, and day of the week. These features can help the model capture seasonal patterns and temporal dependencies in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datetime features\n",
    "def create_datetime_features(series):\n",
    "    logger.info(\"Creating datetime features...\")\n",
    "    features = {}\n",
    "    for attribute in [\"month\", \"year\", \"day_of_week\"]:\n",
    "        features[attribute] = datetime_attribute_timeseries(series, attribute=attribute)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize and Configure the TFT Model\n",
    "\n",
    "Set up the Temporal Fusion Transformer (TFT) model with specified hyperparameters, including input and output lengths, hidden size, attention heads, dropout rate, batch size, number of epochs, and optimizer settings. The model is configured for quantile regression to enable probabilistic forecasting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 21:50:35,391 - INFO - Creating TFT model...\n"
     ]
    }
   ],
   "source": [
    "# Create TFT model with quantile forecasts\n",
    "logger.info(\"Creating TFT model...\")\n",
    "model = TFTModel(\n",
    "    input_chunk_length=24,\n",
    "    output_chunk_length=7,\n",
    "    hidden_size=64,\n",
    "    lstm_layers=1,\n",
    "    num_attention_heads=4,\n",
    "    dropout=0.1,\n",
    "    batch_size=32,\n",
    "    n_epochs=5,\n",
    "    likelihood=QuantileRegression(quantiles=[0.1, 0.5, 0.9]),\n",
    "    optimizer_kwargs={\"lr\": 1e-3},\n",
    "    add_encoders={\"cyclic\": {\"future\": [\"month\", \"day_of_week\"]}},\n",
    "    pl_trainer_kwargs={\n",
    "        \"accelerator\": \"mps\",\n",
    "        \"devices\": 1,\n",
    "        \"callbacks\": [EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")],\n",
    "        \"gradient_clip_val\": 0.1,\n",
    "    },\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the TFT Model\n",
    "\n",
    "Fit the TFT model on the scaled training data using the provided covariates. Monitor the validation loss to implement early stopping and prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 21:50:35,412 - INFO - Training the model...\n",
      "2024-10-24 21:50:35,691 - INFO - Train dataset contains 54912 samples.\n",
      "2024-10-24 21:50:35,702 - INFO - Time series values are 32-bits; casting model to float32.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name                              | Type                             | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | train_metrics                     | MetricCollection                 | 0      | train\n",
      "1  | val_metrics                       | MetricCollection                 | 0      | train\n",
      "2  | input_embeddings                  | _MultiEmbedding                  | 0      | train\n",
      "3  | static_covariates_vsn             | _VariableSelectionNetwork        | 3.3 K  | train\n",
      "4  | encoder_vsn                       | _VariableSelectionNetwork        | 12.5 K | train\n",
      "5  | decoder_vsn                       | _VariableSelectionNetwork        | 7.0 K  | train\n",
      "6  | static_context_grn                | _GatedResidualNetwork            | 16.8 K | train\n",
      "7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 16.8 K | train\n",
      "8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 16.8 K | train\n",
      "9  | static_context_enrichment         | _GatedResidualNetwork            | 16.8 K | train\n",
      "10 | lstm_encoder                      | LSTM                             | 33.3 K | train\n",
      "11 | lstm_decoder                      | LSTM                             | 33.3 K | train\n",
      "12 | post_lstm_gan                     | _GateAddNorm                     | 8.4 K  | train\n",
      "13 | static_enrichment_grn             | _GatedResidualNetwork            | 20.9 K | train\n",
      "14 | multihead_attn                    | _InterpretableMultiHeadAttention | 10.4 K | train\n",
      "15 | post_attn_gan                     | _GateAddNorm                     | 8.4 K  | train\n",
      "16 | feed_forward_block                | _GatedResidualNetwork            | 16.8 K | train\n",
      "17 | pre_output_gan                    | _GateAddNorm                     | 8.4 K  | train\n",
      "18 | output_layer                      | Linear                           | 195    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "229 K     Trainable params\n",
      "0         Non-trainable params\n",
      "229 K     Total params\n",
      "0.920     Total estimated model params size (MB)\n",
      "348       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95c64ce1a2e490ca13b8da0a6c5808f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f24b7a72e894fbdab69597c8e40546d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0ad384a1714ef3be586f30190c68ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066228c923e1402c9ad4f7885600f32c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0523f42d2b744062a560bc6cf8fd502d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9afe155ec3ed48ffa2b02d4d4babe585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160cb659d74b41f9badcff648f5c5297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TFTModel(output_chunk_shift=0, hidden_size=64, lstm_layers=1, num_attention_heads=4, full_attention=False, feed_forward=GatedResidualNetwork, dropout=0.1, hidden_continuous_size=8, categorical_embedding_sizes=None, add_relative_index=False, loss_fn=None, likelihood=QuantileRegression(quantiles: Optional[List[float]] = None), norm_type=LayerNorm, use_static_covariates=True, input_chunk_length=24, output_chunk_length=7, batch_size=32, n_epochs=5, optimizer_kwargs={'lr': 0.001}, add_encoders={'cyclic': {'future': ['month', 'day_of_week']}}, pl_trainer_kwargs={'accelerator': 'mps', 'devices': 1, 'callbacks': [<pytorch_lightning.callbacks.early_stopping.EarlyStopping object at 0x176ac7370>], 'gradient_clip_val': 0.1}, random_state=42)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "logger.info(\"Training the model...\")\n",
    "model.fit(\n",
    "    series=train_scaled,\n",
    "    past_covariates=train_covariates_scaled,\n",
    "    val_series=val_scaled,\n",
    "    val_past_covariates=val_covariates_scaled,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Probabilistic Predictions\n",
    "\n",
    "Use the trained TFT model to make probabilistic forecasts on the validation set. Generate multiple samples to estimate prediction intervals (e.g., 10%, 50%, 90% quantiles).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 22:07:51,469 - INFO - Making predictions...\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954288c1d20341b8b143364abe096000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make probabilistic predictions\n",
    "logger.info(\"Making predictions...\")\n",
    "predictions_90 = model.predict(\n",
    "    n=7,\n",
    "    series=val_scaled,\n",
    "    past_covariates=val_covariates_scaled,\n",
    "    num_samples=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Backtesting\n",
    "\n",
    "Evaluate the model’s performance over historical data by performing backtesting. This involves generating forecasts at multiple points in the validation set to assess consistency and reliability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 22:07:57,494 - INFO - Performing backtesting...\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd966b853ff4c57bb3c548ecc055ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Backtesting\n",
    "logger.info(\"Performing backtesting...\")\n",
    "backtest = model.historical_forecasts(\n",
    "    series=val_scaled,\n",
    "    past_covariates=val_covariates_scaled,\n",
    "    start=0.5,\n",
    "    forecast_horizon=7,\n",
    "    stride=1,\n",
    "    retrain=False,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the TFT Model\n",
    "\n",
    "Utilize Darts’ TFTExplainer to interpret the model’s predictions. This includes analyzing feature importance and attention mechanisms to understand how the model makes decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 22:08:04,566 - INFO - Creating TFT explainer...\n",
      "2024-10-24 22:08:04,567 - ERROR - ValueError: `background_series` must be provided `model` was fit on multiple time series.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`background_series` must be provided `model` was fit on multiple time series.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create TFT explainer\u001b[39;00m\n\u001b[1;32m      2\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating TFT explainer...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m explainer \u001b[38;5;241m=\u001b[39m \u001b[43mTFTExplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m explainability_result \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mexplain()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/env_darts_031/lib/python3.9/site-packages/darts/explainability/tft_explainer.py:108\u001b[0m, in \u001b[0;36mTFTExplainer.__init__\u001b[0;34m(self, model, background_series, background_past_covariates, background_future_covariates)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     54\u001b[0m     model: TFTModel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m     ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     62\u001b[0m ):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    Explainer class for the `TFTModel`.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    >>> explainer.plot_variable_selection(results)\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackground_series\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackground_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackground_past_covariates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackground_past_covariates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackground_future_covariates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackground_future_covariates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequires_background\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequires_covariates_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_component_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_stationarity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# add the relative index that generated inside the model (not in the input data)\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39madd_relative_index:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/env_darts_031/lib/python3.9/site-packages/darts/explainability/explainability.py:92\u001b[0m, in \u001b[0;36m_ForecastingModelExplainer.__init__\u001b[0;34m(self, model, background_series, background_past_covariates, background_future_covariates, requires_background, requires_covariates_encoding, check_component_names, test_stationarity)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_chunk_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# check background input validity and process it\u001b[39;00m\n\u001b[1;32m     84\u001b[0m (\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackground_series,\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackground_past_covariates,\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackground_future_covariates,\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_components,\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic_covariates_components,\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpast_covariates_components,\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture_covariates_components,\n\u001b[0;32m---> 92\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackground\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackground_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_covariates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackground_past_covariates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_covariates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackground_future_covariates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfallback_series\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfallback_past_covariates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpast_covariate_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfallback_future_covariates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfuture_covariate_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_component_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_component_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequires_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequires_background\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequires_covariates_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequires_covariates_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_stationarity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_stationarity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_foreground \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackground_series \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_covariates_encoding \u001b[38;5;241m=\u001b[39m requires_covariates_encoding\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/env_darts_031/lib/python3.9/site-packages/darts/explainability/utils.py:101\u001b[0m, in \u001b[0;36mprocess_input\u001b[0;34m(model, input_type, series, past_covariates, future_covariates, fallback_series, fallback_past_covariates, fallback_future_covariates, check_component_names, requires_input, requires_covariates_encoding, test_stationarity)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m requires_input \u001b[38;5;129;01mand\u001b[39;00m fallback_series \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`model` was fit on multiple time series.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m input_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackground\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno `background_series` was provided at `Explainer` creation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m     )\n\u001b[0;32m--> 101\u001b[0m     \u001b[43mraise_log\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;167;43;01mValueError\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m`\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43minput_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_series` must be provided \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43merror_msg\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m series \u001b[38;5;241m=\u001b[39m fallback_series\n\u001b[1;32m    106\u001b[0m past_covariates \u001b[38;5;241m=\u001b[39m fallback_past_covariates\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/env_darts_031/lib/python3.9/site-packages/darts/logging.py:132\u001b[0m, in \u001b[0;36mraise_log\u001b[0;34m(exception, logger)\u001b[0m\n\u001b[1;32m    129\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exception)\n\u001b[1;32m    130\u001b[0m logger\u001b[38;5;241m.\u001b[39merror(exception_type \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m message)\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: `background_series` must be provided `model` was fit on multiple time series."
     ]
    }
   ],
   "source": [
    "# Create TFT explainer\n",
    "logger.info(\"Creating TFT explainer...\")\n",
    "explainer = TFTExplainer(model)\n",
    "explainability_result = explainer.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Plotting Functions\n",
    "\n",
    "Create helper functions to visualize predictions, backtesting results, feature importance, and attention weights. Visualization aids in interpreting the model’s performance and behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting functions\n",
    "def plot_predictions(series, predictions, title):\n",
    "    logger.info(f\"Plotting predictions: {title}\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    series.plot(label=\"actual\")\n",
    "    predictions.plot(label=[\"10%\", \"median\", \"90%\"])\n",
    "    plt.axhline(y=0.5, color=\"r\", linestyle=\"--\", label=\"Critical threshold (0.5)\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_backtest(series, backtest_predictions, title):\n",
    "    logger.info(f\"Plotting backtest: {title}\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    series.plot(label=\"actual\")\n",
    "    backtest_predictions.plot(label=\"backtest\")\n",
    "    plt.axhline(y=0.5, color=\"r\", linestyle=\"--\", label=\"Critical threshold (0.5)\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Prediction Results\n",
    "\n",
    "Plot the model’s predictions alongside actual values to assess forecast accuracy. Additionally, visualize backtesting results to evaluate model consistency over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "logger.info(\"Plotting results...\")\n",
    "plot_predictions(\n",
    "    val_list[0],\n",
    "    pipeline_main.inverse_transform(predictions_90[0]),\n",
    "    \"TFT Predictions with Confidence Intervals\",\n",
    ")\n",
    "\n",
    "plot_backtest(\n",
    "    val_list[0], pipeline_main.inverse_transform(backtest[0]), \"TFT Backtest Results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Feature Importance and Attention\n",
    "\n",
    "Use the explainer to plot feature importance and attention weights. These plots help in understanding which features the model considers most influential and how it attends to different parts of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance and attention\n",
    "logger.info(\"Plotting feature importance and attention...\")\n",
    "explainer.plot_variable_selection(explainability_result)\n",
    "explainer.plot_attention(explainability_result, plot_type=\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and Display Evaluation Metrics\n",
    "\n",
    "Compute key performance metrics such as Mean Absolute Percentage Error (MAPE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and R-squared (R²) to quantitatively assess the model’s forecasting accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print metrics\n",
    "logger.info(\"Calculating and printing metrics...\")\n",
    "print(\"\\nValidation Metrics:\")\n",
    "print(f\"MAPE: {mape(val_list, pipeline_main.inverse_transform(predictions_90)):.2f}%\")\n",
    "print(f\"RMSE: {rmse(val_list, pipeline_main.inverse_transform(predictions_90)):.2f}\")\n",
    "print(f\"MAE: {mae(val_list, pipeline_main.inverse_transform(predictions_90)):.2f}\")\n",
    "print(f\"R2: {r2_score(val_list, pipeline_main.inverse_transform(predictions_90)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Trained Model\n",
    "\n",
    "Persist the trained TFT model to disk for future use or deployment. Saving the model allows for reproducibility and easy access without retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "logger.info(\"Saving the model...\")\n",
    "model.save(\"best_tft_model\")\n",
    "\n",
    "logger.info(\"Processing completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_darts_031",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
